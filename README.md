# ğŸ® UNO RAG Chatbot

Un chatbot RAG (Retrieval-Augmented Generation) spÃ©cialisÃ© dans les rÃ¨gles du jeu de sociÃ©tÃ© UNO. Le systÃ¨me utilise des modÃ¨les locaux lÃ©gers pour rÃ©pondre aux questions sur les rÃ¨gles du jeu en se basant sur des documents officiels.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![Streamlit](https://img.shields.io/badge/streamlit-1.28+-red.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## ğŸ“‹ FonctionnalitÃ©s

- ğŸ¤– **LLM Local** : Utilise Mistral 7B via Ollama pour des rÃ©ponses de qualitÃ©
- ğŸ“š **RAG Pipeline** : RÃ©cupÃ©ration de contexte pertinent avec FAISS vectorstore
- ğŸ¯ **Embeddings lÃ©gers** : ModÃ¨le sentence-transformers (all-MiniLM-L6-v2, ~80MB)
- ğŸ’» **Interface Streamlit** : Interface web simple et Ã©purÃ©e
- ğŸ‘¤ **Mode User** : RÃ©ponses simples et directes
- ğŸ‘¨â€ğŸ’» **Mode Developer** : Affiche le contexte RAG et les sources utilisÃ©es
- ğŸ”’ **100% Local** : Aucune donnÃ©e envoyÃ©e Ã  des API externes

## ğŸ—ï¸ Architecture

```
Pipeline RAG
â”œâ”€â”€ Documents (data/)
â”‚   â””â”€â”€ PDF/Markdown des rÃ¨gles UNO
â”œâ”€â”€ Chunking & Embedding
â”‚   â””â”€â”€ RecursiveCharacterTextSplitter + all-MiniLM-L6-v2
â”œâ”€â”€ Vectorstore
â”‚   â””â”€â”€ FAISS (index local)
â”œâ”€â”€ Retrieval
â”‚   â””â”€â”€ Similarity search (top-k)
â””â”€â”€ Generation
    â””â”€â”€ Mistral 7B (via Ollama)
```

## ğŸš€ Installation

### PrÃ©requis

- Python 3.8+
- [Ollama](https://ollama.ai/) installÃ©
- ~5GB d'espace disque (modÃ¨les + dÃ©pendances)

### 1. Cloner le repository

```bash
git clone https://github.com/HananeAmc/RAG-Chatbot-UNO-.git
cd uno-rag-chatbot
```

### 2. CrÃ©er un environnement virtuel

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 4. Installer et lancer Ollama

#### Windows
```powershell
winget install Ollama.Ollama
```

#### Linux/Mac
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 5. TÃ©lÃ©charger le modÃ¨le Mistral

```bash
ollama pull mistral
```

VÃ©rifier que le modÃ¨le est bien installÃ© :
```bash
ollama list
```

## ğŸ“‚ Structure du projet

```
llm/
â”œâ”€â”€ data/                      # Documents sources (rÃ¨gles UNO)
â”‚   â”œâ”€â”€ uno_rules_1.pdf
â”‚   â”œâ”€â”€ uno_rules_2.md
â”‚   â””â”€â”€ uno_rules_3.pdf
â”œâ”€â”€ vectorstore/              # GÃ©nÃ©rÃ© automatiquement
â”‚   â”œâ”€â”€ index.faiss
â”‚   â””â”€â”€ index.pkl
â”œâ”€â”€ config.py                 # Configuration (modÃ¨les, paramÃ¨tres)
â”œâ”€â”€ rag_pipeline.py          # Logique RAG complÃ¨te
â”œâ”€â”€ app.py                   # Interface Streamlit
â”œâ”€â”€ requirements.txt         # DÃ©pendances Python
â””â”€â”€ README.md               # Documentation
```

## ğŸ¯ Utilisation

### Lancer l'application

```bash
streamlit run app.py
```

L'application sera accessible sur : `http://localhost:8501`

### Premier lancement

Au premier dÃ©marrage, l'application va :
1. âœ… TÃ©lÃ©charger le modÃ¨le d'embeddings (~80MB)
2. âœ… CrÃ©er le vectorstore Ã  partir des documents UNO
3. âœ… Se connecter Ã  Ollama/Mistral

Temps estimÃ© : **2-3 minutes**

Les lancements suivants seront instantanÃ©s (vectorstore dÃ©jÃ  crÃ©Ã©).

### Modes d'utilisation

#### ğŸ‘¤ Mode User
Affiche uniquement la rÃ©ponse gÃ©nÃ©rÃ©e, idÃ©al pour un usage simple.

**Exemple :**
```
Question: Comment jouer un +4?
RÃ©ponse: La carte +4 sauvage peut Ãªtre jouÃ©e Ã  tout moment. 
Le joueur suivant doit piger 4 cartes et passer son tour.
```

#### ğŸ‘¨â€ğŸ’» Mode Developer
Affiche la rÃ©ponse + le contexte RAG + les sources utilisÃ©es.

**Exemple :**
```
Question: Comment jouer un +4?
RÃ©ponse: [...]

ğŸ“š Contexte utilisÃ© (RAG):
[Montre les chunks rÃ©cupÃ©rÃ©s]

ğŸ” Sources rÃ©cupÃ©rÃ©es:
Source 1: uno_rules_3.pdf (page 2)
Source 2: uno_rules_2.md
[...]
```

## âš™ï¸ Configuration

Modifiez `config.py` pour personnaliser le comportement :

```python
# ModÃ¨le d'embeddings
EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# ModÃ¨le LLM (doit Ãªtre installÃ© dans Ollama)
LLM_MODEL = "mistral"
OLLAMA_BASE_URL = "http://localhost:11434"

# ParamÃ¨tres de chunking
CHUNK_SIZE = 800          # Taille des chunks (en caractÃ¨res)
CHUNK_OVERLAP = 100       # Chevauchement entre chunks

# ParamÃ¨tres de retrieval
TOP_K_RESULTS = 3         # Nombre de chunks Ã  rÃ©cupÃ©rer

# Chemins
VECTORSTORE_PATH = "./vectorstore"
DATA_PATH = "./data"
```

### Changer de modÃ¨le LLM

Pour utiliser un autre modÃ¨le Ollama :

```bash
# TÃ©lÃ©charger un modÃ¨le
ollama pull phi3
# ou
ollama pull llama2

# Modifier config.py
LLM_MODEL = "phi3"  # ou "llama2"
```

**ModÃ¨les recommandÃ©s :**
- `mistral` (7B) - â­ RecommandÃ© - Excellent Ã©quilibre performance/taille
- `phi3:mini` (3.8B) - Plus lÃ©ger, bonnes performances
- `llama2` (7B) - Alternative Ã  Mistral
- `gemma:7b` (7B) - TrÃ¨s bon pour les instructions

## ğŸ”„ RegÃ©nÃ©rer le vectorstore

Si vous modifiez les documents ou les paramÃ¨tres de chunking :

```bash
# Windows
Remove-Item -Recurse -Force .\vectorstore

# Linux/Mac
rm -rf ./vectorstore
```

Le vectorstore sera recrÃ©Ã© au prochain lancement.

## ğŸ› ï¸ Troubleshooting

### Erreur : "Could not connect to Ollama"

**Solution :**
```bash
# VÃ©rifier qu'Ollama est lancÃ©
ollama list

# Si non lancÃ©, dÃ©marrer Ollama
# Windows : Lancer l'application Ollama depuis le menu dÃ©marrer
# Linux/Mac : ollama serve
```

### Erreur : "Model not found: mistral"

**Solution :**
```bash
ollama pull mistral
```

### Vectorstore corrompu

**Solution :**
```bash
Remove-Item -Recurse -Force .\vectorstore
```

### RÃ©ponses lentes

**Solutions :**
- Utiliser un modÃ¨le plus lÃ©ger (`phi3:mini`)
- RÃ©duire `CHUNK_SIZE` dans `config.py`
- RÃ©duire `TOP_K_RESULTS` Ã  2

## ğŸ“Š Performance

| ModÃ¨le | Taille | Vitesse | QualitÃ© |
|--------|--------|---------|---------|
| Mistral 7B | ~4.1GB | ğŸŸ¢ Moyenne | â­â­â­â­â­ Excellente |
| Phi3:mini | ~2.3GB | ğŸŸ¢ Rapide | â­â­â­â­ TrÃ¨s bonne |
| Llama2 7B | ~3.8GB | ğŸŸ¡ Moyenne | â­â­â­â­ TrÃ¨s bonne |

**Temps de rÃ©ponse moyen :** 3-8 secondes (dÃ©pend du CPU)

## ğŸ§ª Tests

Pour tester le chatbot avec diffÃ©rentes questions :

```
âœ… Comment jouer un +4 ?
âœ… Quand dois-je crier UNO ?
âœ… Combien de cartes au dÃ©but ?
âœ… Comment gagner au UNO ?
âœ… Que fait la carte Reverse ?
```

## ğŸ“¦ DÃ©pendances principales

- **streamlit** : Interface web
- **langchain** : Framework RAG
- **sentence-transformers** : Embeddings
- **faiss-cpu** : Vectorstore
- **pypdf** : Parsing PDF

Voir `requirements.txt` pour la liste complÃ¨te.

## ğŸ¯ Cas d'usage

- ğŸ“– **Apprentissage des rÃ¨gles** : Nouveau joueur dÃ©couvrant UNO
- ğŸ² **RÃ©solution de disputes** : VÃ©rifier une rÃ¨gle pendant une partie
- ğŸ§‘â€ğŸ’» **Exemple RAG** : Template pour crÃ©er d'autres chatbots spÃ©cialisÃ©s
- ğŸ« **Education** : DÃ©monstration de RAG et LLM locaux

## ğŸ”® AmÃ©liorations futures

- [ ] Support de plus de formats (DOCX, TXT)
- [ ] Interface multi-langues
- [ ] Export de l'historique des questions
- [ ] API REST pour intÃ©gration
- [ ] Support GPU pour infÃ©rence plus rapide
- [ ] Mode conversation (mÃ©moire du contexte)

## ğŸ“„ Licence

MIT License - Voir [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amelioration`)
3. Commit vos changements (`git commit -m 'Ajout fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/amelioration`)
5. Ouvrir une Pull Request

## ğŸ‘¨â€ğŸ’» Auteur

Votre nom - [@votre-username](https://github.com/votre-username)

## ğŸ™ Remerciements

- [Ollama](https://ollama.ai/) pour l'infrastructure LLM locale
- [LangChain](https://www.langchain.com/) pour le framework RAG
- [Streamlit](https://streamlit.io/) pour l'interface web
- Mattel pour les rÃ¨gles officielles du jeu UNO

---

â­ Si ce projet vous a aidÃ©, n'hÃ©sitez pas Ã  mettre une Ã©toile !
